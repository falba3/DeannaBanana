<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Fundamental Statistics Cheat Sheet</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.4; margin: 20px; }
        h1 { background-color: #003366; color: white; padding: 10px; }
        h2 { color: #003366; border-bottom: 2px solid #003366; padding-bottom: 4px; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
        th, td { border: 1px solid #aaa; padding: 6px; text-align: left; }
        th { background-color: #e6f2ff; }
        .formula { background-color: #f9f9f9; padding: 4px; display: block; margin: 6px 0; }
        .note { color: #333333; font-size: 0.95em; margin: 4px 0; }
    </style>
</head>
<body>

<h1>Fundamental Statistics Cheat Sheet</h1>

<h2>1. Notation & Preliminaries</h2>
<ul>
  <li><strong>Population vs Sample:</strong> Population parameters (e.g. mean μ, proportion p, variance σ²), Sample statistics (e.g. sample mean \(\bar x\), sample proportion \(\hat p\), sample variance s²).</li>
  <li><strong>Common symbols:</strong> \(n\) = sample size, \(\bar x\) = sample mean, \(s\) = sample standard deviation, \(\sigma\) = population standard deviation (if known), \(\hat p\) = sample proportion, \(p\) = population proportion (unknown), \(\alpha\) = significance level, \(1 - \alpha\) (e.g. 0.95) = confidence level.</li>
  <li><strong>When to use Z vs t:</strong>
    <ul>
      <li>Z-tests / Z-intervals: when population σ is known, or for large samples (by CLT). :contentReference[oaicite:0]{index=0}</li>
      <li>t-tests / t-intervals: when σ is unknown and sample size is small; uses sample s instead. :contentReference[oaicite:1]{index=1}</li>
    </ul>
  </li>
  <li><strong>Assumptions (typical):</strong> independent observations; for mean-based inference → population roughly normal or large \(n\) (by Central Limit Theorem). :contentReference[oaicite:2]{index=2}</li>
</ul>

<h2>2. One-Population Inference</h2>

<h3>2.1 Confidence Interval for a Population Mean</h3>
<p>If you have sample mean \(\bar x\), want a confidence interval for population mean μ:</p>
<ul>
  <li><strong>When σ known (rare):</strong>
    <span class="formula">\(\bar x \pm z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}\)</span>
  </li>
  <li><strong>When σ unknown (usual case):</strong>
    <span class="formula">\(\bar x \pm t_{\alpha/2,\,df} \cdot \frac{s}{\sqrt{n}}\)</span> where \(df = n - 1\). :contentReference[oaicite:3]{index=3}</li>
</ul>
<p class="note">Interpretation: If you repeated sampling many times, approximately \((1-\alpha)\)% of intervals would contain the true population mean μ. :contentReference[oaicite:4]{index=4}</p>

<h3>2.2 Confidence Interval for a Population Proportion</h3>
<p>For sample proportion \(\hat p\), confidence interval for population proportion \(p\):</p>
<span class="formula">\(\hat p \pm z_{\alpha/2} \cdot \sqrt{\frac{\hat p\, (1 - \hat p)}{n}}\)</span> :contentReference[oaicite:5]{index=5}  
<p class="note">Works when sample size is large enough (so that sampling distribution of \(\hat p\) is approximately normal). :contentReference[oaicite:6]{index=6}</p>

<h3>2.3 Hypothesis Testing for Population Mean</h3>
<p>Test \(H_0: \mu = \mu_0\) vs alternative (e.g. ≠, >, <):</p>
<ul>
  <li><strong>Z-test</strong> (σ known or large \(n\)):  
    <span class="formula">\(z = \dfrac{\bar x - \mu_0}{\sigma / \sqrt{n}}\)</span>  
  </li>
  <li><strong>t-test</strong> (σ unknown):  
    <span class="formula">\(t = \dfrac{\bar x - \mu_0}{s / \sqrt{n}}\)</span>, with \(df = n - 1\). :contentReference[oaicite:7]{index=7}</li>
</ul>
<p class="note">Compare test statistic to critical value (from Z or t tables) or compute p-value. If p-value < α → reject \(H_0\). :contentReference[oaicite:8]{index=8}</p>

<h3>2.4 Hypothesis Testing for Population Proportion</h3>
<p>Test \(H_0: p = p_0\) vs alternative. Use:</p>
<span class="formula">\(z = \dfrac{\hat p - p_0}{\sqrt{\dfrac{p_0 (1 - p_0)}{n}}}\)</span> :contentReference[oaicite:9]{index=9}  
<p class="note">Assumes large enough sample so sampling distribution approximates normal. Use continuity corrections or exact tests if not. :contentReference[oaicite:10]{index=10}</p>

<h2>3. Two-Population Inference (Comparing Two Groups)</h2>

<h3>3.1 Independent Samples — Difference of Means</h3>
<p>You have two independent samples (size \(n_1\), \(n_2\)), sample means \(\bar x_1, \bar x_2\), sample (or population) variances. Two cases:</p>
<ul>
  <li><strong>Equal variances (pooled):</strong>  
    <span class="formula">\((\bar x_1 - \bar x_2) \pm t_{\alpha/2, df} \cdot s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\)</span>, where \( s_p^2 = \dfrac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2} \).  
  </li>
  <li><strong>Unequal variances (“Welch’s”):</strong>  
    <span class="formula">\((\bar x_1 - \bar x_2) \pm t_{\alpha/2, df^*} \cdot \sqrt{\dfrac{s_1^2}{n_1} + \dfrac{s_2^2}{n_2}}\)</span>, with approximate \(df^*\).  
  </li>
</ul>
<p class="note">Use pooled only if variances roughly equal; otherwise use Welch’s. Many modern sources prefer Welch’s by default. :contentReference[oaicite:11]{index=11}</p>

<h3>3.2 Paired Samples (Paired Means)</h3>
<p>When you have “before vs after” or matched data: compute differences \(d_i = x_{1i} - x_{2i}\); then treat as single sample of differences. Use:</p>
<span class="formula">\( \bar d \pm t_{\alpha/2,\,df} \cdot \dfrac{s_d}{\sqrt{n}} \)</span>  
and for test statistic:  
<span class="formula">\( t = \dfrac{\bar d - d_0}{s_d / \sqrt n} \)</span> (often \(d_0 = 0\)).  

<h3>3.3 Difference of Proportions (Two-Sample Proportions)</h3>
<p>For two independent samples with sizes \(n_1, n_2\), success counts \(x_1, x_2\), sample proportions \(\hat p_1 = x_1/n_1\), \(\hat p_2 = x_2/n_2\):</p>
<ul>
  <li><strong>Confidence interval for difference:</strong>  
    <span class="formula">\(\hat p_1 - \hat p_2 \pm z_{\alpha/2} \cdot \sqrt{\dfrac{\hat p_1 (1 - \hat p_1)}{n_1} + \dfrac{\hat p_2 (1 - \hat p_2)}{n_2}}\)</span> :contentReference[oaicite:12]{index=12}</li>
  <li><strong>Hypothesis test (two-proportion Z-test):</strong> Under \(H_0: p_1 = p_2\), define pooled proportion \(\hat p = \dfrac{x_1 + x_2}{n_1 + n_2}\), then  
    <span class="formula">\( z = \dfrac{\hat p_1 - \hat p_2}{\sqrt{\hat p (1 - \hat p) \left( \dfrac{1}{n_1} + \dfrac{1}{n_2} \right)}} \)</span> :contentReference[oaicite:13]{index=13}</li>
</ul>

<h2>4. Analysis of Variance (ANOVA)</h2>
<p>Use ANOVA when comparing means across three or more groups (or populations). It examines whether group means differ more than would be expected by chance. :contentReference[oaicite:14]{index=14}</p>

<h3>One-Way ANOVA (basic idea)</h3>
<ul>
  <li><strong>Null hypothesis \(H_0\):</strong> All group means are equal (e.g. \(\mu_1 = \mu_2 = \cdots = \mu_k\)).</li>
  <li><strong>Alternative \(H_a\):</strong> At least one group mean differs.</li>
  <li><strong>F-statistic:</strong>  
    <span class="formula">\( F = \dfrac{\text{Between-group variability (mean squares)}}{\text{Within-group variability (mean squares)}} \)</span></li>
  <li>If \(F\) is large (exceeds critical value), reject \(H_0\). Significance implies difference somewhere — but doesn’t tell which groups differ (post-hoc tests needed).</li>
</ul>
<p class="note">ANOVA reduces risk of Type I error compared to doing many separate t-tests. :contentReference[oaicite:15]{index=15}</p>

<h2>5. Interpreting Correlation & Scatterplots</h2>
<p>The most common correlation measure: Pearson correlation coefficient \(r\), which quantifies the linear association between two continuous variables X and Y.</p>
<ul>
  <li><strong>Definition (sample):</strong>  
    <span class="formula">\( r = \dfrac{ \sum (x_i - \bar x)(y_i - \bar y)} { \sqrt{ \sum (x_i - \bar x)^2 \; \sum (y_i - \bar y)^2 } } \)</span></li>
  <li><strong>Interpretation:</strong>
    <ul>
      <li>Sign: positive → as X increases, Y tends to increase; negative → as X increases, Y tends to decrease.</li>
      <li>Magnitude (absolute value):  
        <ul>
          <li>|r| close to 1 → strong linear relationship</li>
          <li>|r| close to 0 → weak or no linear relationship</li>
        </ul>
      </li>
      <li>Important: correlation does **not** imply causation.</li>
    </ul>
  </li>
  <li><strong>Limitations:</strong> Sensitive to outliers; only captures linear association; doesn’t reflect non-linear relationships.</li>
</ul>

<h2>6. Reading and Interpreting Histograms</h2>
<p>A histogram displays the distribution of a single continuous (or discrete but many values) variable by grouping data into “bins” and showing the frequency (or relative frequency) in each bin. Key insights:</p>
<ul>
  <li><strong>Shape:</strong> Symmetric, skewed (left/right), multimodal, etc.</li>
  <li><strong>Center:</strong> Where data tends to cluster (mean, median approximate).</li>
  <li><strong>Spread / Variability:</strong> How wide the distribution is — range, variability, outliers.</li>
  <li><strong>Skewness:</strong> Right-skewed (long right tail) or left-skewed (long left tail).</li>
  <li><strong>Modality:</strong> Unimodal, bimodal, multimodal — may indicate subpopulations or different processes.</li>
  <li><strong>Outliers / Gaps / Clusters:</strong> Aberrant values, gaps, clusters — may suggest measurement issues or distinct groups.</li>
</ul>
<p class="note">Histograms are useful to check assumptions (e.g. normality), detect outliers, and understand the distribution before applying statistical inference. (Often done before t-tests/ANOVA, etc.)</p>

<h2>7. Discrete Distributions: Binomial & Poisson</h2>

<h3>7.1 Binomial Distribution</h3>
<p>Used for a fixed number \(n\) of independent Bernoulli trials (each with success probability \(p\)). The random variable \(X =\) number of successes ~ Binomial(\(n, p\)).</p>
<ul>
  <li><strong>PMF (probability mass function):</strong>  
    <span class="formula">\( P(X = k) = {n \choose k} \; p^k \; (1 - p)^{n - k} \), for \(k = 0, 1, \dots, n\). </span></li>
  <li><strong>Mean & Variance:</strong>  
    <span class="formula">\( E[X] = n p,\quad \mathrm{Var}(X) = n p (1 - p) \).</span></li>
  <li><strong>Use:</strong> modeling number of successes in fixed number of trials; yes/no events; computing exact probabilities.</li>
  <li><strong>Confidence intervals for proportion:</strong> converting to proportion \(\hat p = X/n\) and use proportion-CI formulas. :contentReference[oaicite:16]{index=16}</li>
</ul>

<h3>7.2 Poisson Distribution</h3>
<p>Used to model counts of events in a fixed interval of time/space, under assumptions: events happen independently, at a constant average rate \(\lambda\), and rarely relative to interval length.</p>
<ul>
  <li><strong>PMF:</strong>  
    <span class="formula">\( P(X = k) = \dfrac{e^{-\lambda} \; \lambda^k}{k!} \), for \(k = 0,1,2,\dots\).</span></li>
  <li><strong>Mean & Variance:</strong>  
    <span class="formula">\( E[X] = \lambda, \; \mathrm{Var}(X) = \lambda. \)</span></li>
  <li><strong>Use:</strong> rare events over time/space (e.g. number of accidents, calls at call center, rare disease occurrences).</li>
  <li><strong>Confidence interval for \(\lambda\):</strong> there is an “exact” CI using the relation with chi-squared distribution:  
    <span class="formula">\( \dfrac{1}{2} \chi^2_{\alpha/2; 2k} \; \le \; \lambda \; \le \; \dfrac{1}{2} \chi^2_{1-\alpha/2; 2k+2} \)  if we observed \(k\) events. :contentReference[oaicite:17]{index=17}</span></li>
  <li><strong>Relation to Binomial:</strong> For large \(n\), small \(p\), Binomial(\(n, p\)) ≈ Poisson(\(\lambda = n p\)).</li>
</ul>

<h2>8. Quick Summary — Formula Table</h2>

<table>
  <thead>
    <tr><th>Inference / Distribution</th><th>Formula / Key Expression</th><th>When to use / Notes</th></tr>
  </thead>
  <tbody>
    <tr><td>CI for mean (σ known)</td><td>\(\bar x \pm z_{\alpha/2} \cdot \dfrac{\sigma}{\sqrt n}\)</td><td>Rare, when population σ known or large n</td></tr>
    <tr><td>CI for mean (σ unknown)</td><td>\(\bar x \pm t_{\alpha/2,\,df} \cdot \dfrac{s}{\sqrt n}\)</td><td>Common — sample mean, unknown σ</td></tr>
    <tr><td>CI for proportion</td><td>\(\hat p \pm z_{\alpha/2} \cdot \sqrt{\dfrac{\hat p (1-\hat p)}{n}}\)</td><td>\(n\) large enough</td></tr>
    <tr><td>One-sample z-test (mean)</td><td>\(z = \dfrac{\bar x - \mu_0}{\sigma / \sqrt n}\)</td><td>σ known or large \(n\)</td></tr>
    <tr><td>One-sample t-test (mean)</td><td>\(t = \dfrac{\bar x - \mu_0}{s / \sqrt n}\)</td><td>σ unknown, small/medium \(n\)</td></tr>
    <tr><td>One-sample z-test (proportion)</td><td>\(z = \dfrac{\hat p - p_0}{\sqrt{\dfrac{p_0 (1 - p_0)}{n}}}\)</td><td>Large sample, binary data</td></tr>
    <tr><td>Two-sample CI (means, independent)</td><td>\((\bar x_1 - \bar x_2) \pm t* \cdot \mathrm{SE}\)</td><td>See section on pooled / Welch</td></tr>
    <tr><td>Two-sample z-test (proportions)</td><td>\(z = \dfrac{\hat p_1 - \hat p_2}{\sqrt{\hat p (1-\hat p) (\tfrac1{n_1} + \tfrac1{n_2})}}\)</td><td>Binary, independent samples, large n</td></tr>
    <tr><td>Binomial PMF</td><td \> \({n \choose k} p^k (1-p)^{n-k}\)</td><td>Number of successes in fixed trials</td></tr>
    <tr><td>Poisson PMF</td><td>\( e^{-\lambda} \lambda^k / k! \)</td><td>Counts over interval, rare events</td></tr>
  </tbody>
</table>

<h2>9. Recommendations / Good Practices & Warnings</h2>
<ul>
  <li><strong>Check assumptions:</strong> independence, sample size, normality (or rely on CLT), equal variances when pooling, etc.</li>
  <li><strong>Avoid blindly using normal-based CIs / tests for small samples or skewed data.</li>
  <li><strong>Be cautious with proportions and binary data:</strong> for small sample / extreme p, normal approximation may fail; consider exact or adjusted methods. :contentReference[oaicite:18]{index=18}</li>
  <li><strong>Don’t over-interpret correlation:</strong> correlation ≠ causation; be aware of outliers; correlation only captures linear trends.</li>
  <li><strong>When comparing many groups:</strong> use ANOVA, avoid multiple t-tests to reduce Type I error inflation. :contentReference[oaicite:19]{index=19}</li>
</ul>

</body>
</html>
